---
layout: post
title: "How do Neural Networks Learn?"
subtitle: "Deciphering the inner workings on neural networks"
katex: True
comments: True
---



Next you can update your site name, avatar and other options using the _config.yml file in the root of your repository (shown below). test drop pagination text chunk preview.

![_config.yml]({{ site.baseurl }}/images/config.png)

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.


#### The scaling hypothesis and the laziness of deep learning

The scaling hypothesis is that

> we can simply train ever larger NNs and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks & data

Gwern cites a swathe of papers in support, interpreting them in such a way that the following picture emerges:

#### The scaling hypothesis and the laziness of deep learning

The scaling hypothesis is that

> we can simply train ever larger NNs and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks & data

Gwern cites a swathe of papers in support, interpreting them in such a way that the following picture emerges:
